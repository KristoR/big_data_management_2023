{"cells":[{"cell_type":"markdown","source":["### Task 1\n\nUsing the Chicago crimes dataset from above, create:\n1. an RDD holding a distinct list of the column \"Primary type\" (30 rows total including heading)\n2. an RDD holding the same distinct list, together with a count of each primary type, eg: ('HOMICIDE', 48)\n\nThe dataset can be found at \"mnt/training/Chicago-Crimes-2018.csv\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1370dcc-a196-47dc-998a-e86a89780d29"}}},{"cell_type":"code","source":["# first RDD\n\ncrimes_rdd = sc.textFile(\"mnt/training/Chicago-Crimes-2018.csv\")\n\n# You can use this to make sure which column is \"Primary type\". We find that it is the 6th column.\n# (crimes_rdd\n#  .map(lambda x: x.split(\"\\t\"))\n#  .take(1)\n# )\n\n\n# Create the RDD (remember, if we trigger any kind of action, then it is no longer an RDD)\ndistinct_list_rdd = (crimes_rdd\n .map(lambda x: x.split(\"\\t\")[5]) # As the split() function returns a list, we can use slicing directly on top of it to only get the column that we are interested in\n .distinct()\n)\n\n# Validate the count of distinct values in this column\ncount_of_distinct = distinct_list_rdd.count()\n\nprint(f\"Distinct rows including heading: {count_of_distinct}\")\n\n# Validate that the type is RDD\nprint(f\"The type is: {type(distinct_list_rdd)}\")\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a49c1ca-0870-40f0-81fd-7b63093e8f16"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Distinct rows including heading: 30\nThe type is: <class 'pyspark.rdd.PipelinedRDD'>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Distinct rows including heading: 30\nThe type is: <class 'pyspark.rdd.PipelinedRDD'>\n"]}}],"execution_count":0},{"cell_type":"code","source":["# second RDD\n\n# Create the RDD\nrdd_with_counts = (crimes_rdd\n .map(lambda x: x.split(\"\\t\")[5])\n .map(lambda x: (x,1)) # x is string type. We now turn it into a tuple, where we hold the string (as key) and integer 1 (as value) for summing up counts.\n .reduceByKey(lambda x,y: x+y) # we group based on key (string) and sum all values where the key is the same\n )\n\n# Let's validate how it looks:\nrdd_with_counts.take(30)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcc900bc-9786-45fd-8182-98dfe8d6d390"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[2]: [('Primary Type', 1),\n ('HOMICIDE', 48),\n ('BURGLARY', 1037),\n ('BATTERY', 4076),\n ('CRIMINAL DAMAGE', 2178),\n ('ROBBERY', 1018),\n ('KIDNAPPING', 19),\n ('WEAPONS VIOLATION', 439),\n ('NARCOTICS', 1050),\n ('CRIMINAL TRESPASS', 676),\n ('PUBLIC PEACE VIOLATION', 96),\n ('LIQUOR LAW VIOLATION', 27),\n ('PROSTITUTION', 36),\n ('OBSCENITY', 9),\n ('NON-CRIMINAL', 3),\n ('HUMAN TRAFFICKING', 1),\n ('DECEPTIVE PRACTICE', 1380),\n ('THEFT', 5247),\n ('ASSAULT', 1624),\n ('OTHER OFFENSE', 1490),\n ('OFFENSE INVOLVING CHILDREN', 196),\n ('MOTOR VEHICLE THEFT', 1138),\n ('CRIM SEXUAL ASSAULT', 115),\n ('STALKING', 17),\n ('SEX OFFENSE', 47),\n ('INTERFERENCE WITH PUBLIC OFFICER', 99),\n ('ARSON', 29),\n ('GAMBLING', 5),\n ('INTIMIDATION', 4),\n ('CONCEALED CARRY LICENSE VIOLATION', 4)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[2]: [('Primary Type', 1),\n ('HOMICIDE', 48),\n ('BURGLARY', 1037),\n ('BATTERY', 4076),\n ('CRIMINAL DAMAGE', 2178),\n ('ROBBERY', 1018),\n ('KIDNAPPING', 19),\n ('WEAPONS VIOLATION', 439),\n ('NARCOTICS', 1050),\n ('CRIMINAL TRESPASS', 676),\n ('PUBLIC PEACE VIOLATION', 96),\n ('LIQUOR LAW VIOLATION', 27),\n ('PROSTITUTION', 36),\n ('OBSCENITY', 9),\n ('NON-CRIMINAL', 3),\n ('HUMAN TRAFFICKING', 1),\n ('DECEPTIVE PRACTICE', 1380),\n ('THEFT', 5247),\n ('ASSAULT', 1624),\n ('OTHER OFFENSE', 1490),\n ('OFFENSE INVOLVING CHILDREN', 196),\n ('MOTOR VEHICLE THEFT', 1138),\n ('CRIM SEXUAL ASSAULT', 115),\n ('STALKING', 17),\n ('SEX OFFENSE', 47),\n ('INTERFERENCE WITH PUBLIC OFFICER', 99),\n ('ARSON', 29),\n ('GAMBLING', 5),\n ('INTIMIDATION', 4),\n ('CONCEALED CARRY LICENSE VIOLATION', 4)]"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Task 2\n\nUsing the jsonplaceholder data, create an RDD that would return username and proportion (decimal percentage) of todos done.\n\nEndpoints:\n* https://jsonplaceholder.typicode.com/users\n* https://jsonplaceholder.typicode.com/todos\n\nExpected output is a sorted RDD, descending by proportion.</br>\nExample take(3):</br>\n[('Moriah.Stanton', 0.6),</br>\n ('Kamren', 0.6),</br>\n ('Maxime_Nienow', 0.55)]</br>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a29f9ff0-683a-4ec4-930d-97fa1dcc8239"}}},{"cell_type":"code","source":["import requests\n\nusers_path = \"https://jsonplaceholder.typicode.com/users\"\ntodos_path = \"https://jsonplaceholder.typicode.com/todos\"\n\nusers_resp = requests.get(users_path)\ntodos_resp = requests.get(todos_path)\n\nusers_rdd = sc.parallelize(users_resp.json())\ntodos_rdd = sc.parallelize(todos_resp.json())\n\n# Let's see how the data looks\n#users_rdd.take(1) # What we need is the \"username\" field, probably something for joining - most likely the id field\n#todos_rdd.take(1) # What we need here is the \"completed\" field. For joining, the correct field is probably \"userId\"\n\n# We can create the RDDs in steps, e.g.:\n# users_rdd_for_joining = users_rdd.map(lambda x: (x[\"id\"], x[\"username\"]))\n# todos_rdd_for_joining = todos_rdd.map(lambda x: (x[\"userId\"], x[\"completed\"]))\n\n# This is good practice if you are just setting up your pipeline. \n\n# Probably, in the longterm you want something more concise. So, you would write everything together:\ntodos_done_rdd = (users_rdd.map(lambda x: (x[\"id\"], x[\"username\"]))\n .join(\n    todos_rdd.map(lambda x: (x[\"userId\"], x[\"completed\"]))\n  ) # we now have a tuple (key-value), where the first element is \"userId\", the second element is a tuple of \"username\" and \"completed\"\n .map(lambda x: (x[1][0],(1, 1)) if x[1][1] else (x[1][0], (0, 1))) # we create a new tuple, with username as key, and value as a tuple with \"completed\" (1=True, 0=False) and count\n .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) # we group the values together, so now we have the total count of completed tasks and the total amount of tasks, per username\n .mapValues(lambda x: x[0]/x[1]) # we divide completed/total to get the proportion\n .sortBy(lambda x: (x[1]), ascending=False) # we sort by the proportion in a descending manner\n)\n\n# And we can validate the result:\ntodos_done_rdd.take(20)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d19fcb6-6590-48c2-a7a1-3dc99d2c420a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: [('Moriah.Stanton', 0.6),\n ('Kamren', 0.6),\n ('Maxime_Nienow', 0.55),\n ('Bret', 0.55),\n ('Elwyn.Skiles', 0.45),\n ('Antonette', 0.4),\n ('Delphine', 0.4),\n ('Samantha', 0.35),\n ('Karianne', 0.3),\n ('Leopoldo_Corkery', 0.3)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: [('Moriah.Stanton', 0.6),\n ('Kamren', 0.6),\n ('Maxime_Nienow', 0.55),\n ('Bret', 0.55),\n ('Elwyn.Skiles', 0.45),\n ('Antonette', 0.4),\n ('Delphine', 0.4),\n ('Samantha', 0.35),\n ('Karianne', 0.3),\n ('Leopoldo_Corkery', 0.3)]"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Practice session - RDD answers","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":256761061374205}},"nbformat":4,"nbformat_minor":0}
