{"cells":[{"cell_type":"markdown","source":["### Delta Lake continued, Intro to Structured Streaming\n\n* Delta Lake\n    * Data Lake to Data Warehouse: merge into\n    * Updating table definition\n    * Partition column\n* Structured Streaming intro"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b131d293-c37a-4753-95e3-85a755db6aaa","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pyspark.sql.functions as F\nfrom delta.tables import *\n\n# We will use a movies dataset and try to simulate dimensional modelling behaviour\n#dbutils.fs.ls(\"mnt/training/movies/1m/\")\n#print(dbutils.fs.head(\"mnt/training/movies/1m/README\"))\n#print(dbutils.fs.head(\"mnt/training/movies/1m/movies.dat\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0cb717b3-8efb-4e42-9fd0-7a1ecefd7e5a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# we don't need to create a separate variable for our dataframes\n# if we have done testing/debugging, we can be more concise\n\n(spark.read\n .option(\"delimiter\", \"::\")\n .option(\"inferSchema\", \"true\")\n .csv(\"/mnt/training/movies/1m/movies.dat\")\n .toDF(\"MovieID\",\"Title\",\"Genres\") # for naming columns\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .saveAsTable(\"source_movies\")\n)\n\n(spark.read\n .option(\"delimiter\", \"::\")\n .option(\"inferSchema\", \"true\")\n .csv(\"/mnt/training/movies/1m/ratings.dat\")\n .toDF(\"UserID\",\"MovieID\",\"Rating\",\"Timestamp\")\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .saveAsTable(\"source_ratings\")\n)\n\n(spark.read\n .option(\"delimiter\", \"::\")\n .option(\"inferSchema\", \"true\")\n .csv(\"/mnt/training/movies/1m/users.dat\")\n .toDF(\"UserID\",\"Gender\",\"Age\",\"Occupation\",\"Zip-code\")\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .saveAsTable(\"source_users\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"29997c12-5edd-4bf4-9d29-e66e6d1c96ea","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's separate data into smaller batches for simulating incremental inserts\n(spark.table(\"source_ratings\")\n .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2000-01-01','2000-12-31'))\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .saveAsTable(\"source_ratings_2000\")\n)\n\n(spark.table(\"source_ratings\")\n .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2001-01-01','2001-12-31'))\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .saveAsTable(\"source_ratings_2001\")\n)\n\n(spark.table(\"source_ratings\")\n .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2002-01-01','2002-12-31'))\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .saveAsTable(\"source_ratings_2002\")\n)\n\n(spark.table(\"source_ratings\")\n .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2003-01-01','2003-12-31'))\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .saveAsTable(\"source_ratings_2003\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"377629c8-db8b-4027-b7e9-8910cfcf6d37","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"\"\"\nCREATE OR REPLACE TABLE DimUser AS\nSELECT *\n, CAST('2001-01-01' as date) ValidFrom\n, CAST('9999-12-31' as date) ValidTo\nFROM source_users\nWHERE UserID IN (SELECT UserID FROM source_ratings_2000)\n\"\"\")\n\nspark.sql(\"\"\"\nCREATE OR REPLACE TABLE DimMovie AS\nSELECT *\n, CAST('2001-01-01' as date) ValidFrom\n, CAST('9999-12-31' as date) ValidTo\nFROM source_movies\nWHERE movieID IN (SELECT movieID FROM source_ratings_2000)\n\"\"\")\n\nspark.sql(\"\"\"\nCREATE OR REPLACE TABLE FactRating AS\nSELECT *\nFROM source_ratings_2000\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ea64d44a-dad2-444f-83ad-65e069472902","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"\"\"UPDATE source_users\nSET Age = 21\nWHERE Age = 1\n\"\"\")\n\nspark.sql(\"\"\"UPDATE source_users\nSET `Zip-code` = 12345\nWHERE `Zip-Code` = 10023\"\"\")\n\nspark.sql(\"\"\"UPDATE source_movies\nSET Genres = 'Comedy'\nWHERE MovieID = 18\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f4d2995-613d-40bb-b626-3c18bf79a93b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"\"\"\nMERGE INTO DimUser tgt\nUSING (\n  SELECT * FROM source_users\n  WHERE UserID IN (SELECT UserID FROM source_ratings_2000)\n  OR UserID IN (SELECT UserID FROM source_ratings_2001)\n) src\nON tgt.UserID = src.UserID\nWHEN MATCHED AND (tgt.Gender != src.Gender OR tgt.Age != src.Age OR tgt.Occupation != src.Occupation OR tgt.`Zip-code` != src.`Zip-code`)\n  THEN UPDATE SET tgt.ValidTo = '2002-01-01'\nWHEN NOT MATCHED THEN INSERT (UserID, Gender, Age, Occupation, `Zip-code`, ValidFrom, ValidTo) VALUES ( \n          src.UserID\n          , src.Gender\n          , src.Age\n          , src.Occupation\n          , src.`Zip-code`\n          , CAST('2002-01-01' as date) \n          , CAST('9999-12-31' as date)\n          )\n\"\"\")\n\nspark.sql(\"\"\"\nMERGE INTO DimUser tgt\nUSING (\n  SELECT * FROM source_users\n  WHERE UserID IN (SELECT UserID FROM source_ratings_2000)\n  OR UserID IN (SELECT UserID FROM source_ratings_2001)\n) src\nON tgt.UserID = src.UserID\nAND tgt.Gender = src.Gender\nAND tgt.Age = src.Age\nAND tgt.Occupation = src.Occupation\nAND tgt.`Zip-code` = src.`Zip-code`\nWHEN NOT MATCHED THEN INSERT (UserID, Gender, Age, Occupation, `Zip-code`, ValidFrom, ValidTo) VALUES ( \n          src.UserID\n          , src.Gender\n          , src.Age\n          , src.Occupation\n          , src.`Zip-code`\n          , CAST('2002-01-01' as date) \n          , CAST('9999-12-31' as date)\n          )\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c9c63108-a666-4927-9727-e87935713bce","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# using the Delta API\n\ndimMovieTable = DeltaTable.forName(spark, \"dimMovie\")\nsourceMoviesDf = spark.sql(\"\"\"\nSELECT * FROM source_movies\nWHERE MovieID IN (SELECT MovieID FROM source_ratings_2000)\nOR MovieID IN (SELECT MovieID FROM source_ratings_2001)\n\"\"\")\n\ndimMovieTable.alias(\"tgt\").merge(\n  source = sourceMoviesDf.alias(\"src\"),\n  condition = \"tgt.MovieID = src.MovieID\"\n).whenMatchedUpdate(\n  condition = \"tgt.Title != src.Title OR tgt.Genres != src.Genres\",\n  set = \n  {\n   \"ValidTo\": \"cast('2002-01-01' as date)\"\n  }\n).whenNotMatchedInsert(values =\n    {\n      \"MovieID\": \"src.MovieID\",\n      \"Title\": \"src.Title\",\n      \"Genres\": \"src.Genres\",\n      \"ValidFrom\": \"cast('2002-01-01' as date)\",\n      \"ValidTo\": \"cast('9999-12-31' as date)\"\n    }\n).execute()\n\n\ndimMovieTable.alias(\"tgt\").merge(\n  source = sourceMoviesDf.alias(\"src\"),\n  condition = \"tgt.MovieID = src.MovieID AND tgt.Title = src.Title AND tgt.Genres = src.Genres\"\n).whenNotMatchedInsert(values =\n    {\n      \"MovieID\": \"src.MovieID\",\n      \"Title\": \"src.Title\",\n      \"Genres\": \"src.Genres\",\n      \"ValidFrom\": \"cast('2002-01-01' as date)\",\n      \"ValidTo\": \"cast('9999-12-31' as date)\"\n    }\n).execute()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"54ea1878-cddb-458d-81d5-a62118127c1f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# schema evolution\n\ndf = (spark.table(\"source_ratings_2001\")\n      .select(\"*\", \n              F.col(\"timestamp\")\n              .cast(\"timestamp\")\n              .cast(\"date\")\n              .alias(\"date\")\n             )\n      .write\n      .format(\"delta\")\n      .mode(\"append\")\n      .option(\"mergeSchema\", \"true\")\n      .saveAsTable(\"FactRating\")\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5c110656-c67a-4fbc-8762-08cb8b911da1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# schema overwriting - mergeSchema is probably not what you want\n\ndf = (spark.table(\"source_ratings\")\n     .select(F.col(\"userID\").alias(\"user\"), \n              F.col(\"timestamp\")\n              .cast(\"timestamp\")\n              .alias(\"utc_timestamp\"), \n              F.col(\"timestamp\")\n              .cast(\"timestamp\")\n              .cast(\"date\")\n              .alias(\"utc_date\")\n             )\n      .write\n      .format(\"delta\")\n      .mode(\"overwrite\")\n      .option(\"mergeSchema\", \"true\")\n      .saveAsTable(\"FactRating\")\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c0cf9819-2e58-4cd6-a950-efd19b0d9e8e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# instead, use overwriteSchema - note, append mode is not allowed\n\n(spark.table(\"source_ratings\")\n .select(F.col(\"userID\").alias(\"user\"), \n          F.col(\"timestamp\")\n          .cast(\"timestamp\")\n          .alias(\"utc_timestamp\"), \n          F.col(\"timestamp\")\n          .cast(\"timestamp\")\n          .cast(\"date\")\n          .alias(\"utc_date\")\n         )\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"overwriteSchema\", \"true\")\n#  .partitionBy(\"utc_date\") # column partition - most commonly on date, needs to be not too unique. Ideally something you use in filters a lot. Only makes sense if it would be ~ 1 GB per partition!\n  .saveAsTable(\"FactRating\")\n )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49e23263-c4eb-4fcf-be33-1dc3c384c36b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# How to create tables from existing Delta data\n#spark.sql(\"CREATE TABLE x USING DELTA LOCATION '/user/hive/warehouse/x'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"72792651-d7e3-45df-9a60-d7086844e07b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Structured streaming\n\nCommon input/output:\n* Input sources\n  * Kafka (and other distributed commit logs)\n  * Files on a distributed system\n  * TCP-IP sockets _note: not fault-tolerant_\n* Output (sinks)\n  * Kafka (etc)\n  * File formats\n  * Spark tables"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d095919-1574-47e6-bd89-58298304e99f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# let's start by looking at reading and writing streams to files\n# we need a schema when reading streams\n\nevents_schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n\nstreaming_events_df = (spark.readStream\n  .schema(events_schema)\n  .option(\"maxFilesPerTrigger\", 1) # used for example purposes, reads in 1 file per trigger\n  .parquet(\"/mnt/training/ecommerce/events/events.parquet\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a000241-a9e5-4dbd-9e47-e3b4db029f33","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# you can check if a dataframe has streaming sources\n# this means some functions are unavailable (eg count)\n\nstreaming_events_df.isStreaming\n\n\n# displaying data\n# in Databricks, we can use display(df) for debugging\n# running locally, you can debug output to console:\n# df.writeStream.format(\"console\").start()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cd55451d-fefc-4712-bffd-a92391082893","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's create a new dataframe and write it into a file\n\nemail_df = (streaming_events_df\n            .filter(F.col(\"traffic_source\") == \"email\")\n            .withColumn(\"mobile\", F.col(\"device\").isin([\"iOS\", \"Android\"]))\n            .select(\"user_id\", \"event_timestamp\", \"mobile\")\n           )\n\ncheckpoint_path = \"/tmp/email_traffic/checkpoint\" \noutput_path = \"/tmp/email_traffic/output\"\n\ndevices_query = (email_df.writeStream\n  .outputMode(\"append\") # append = only new rows, complete = all rows written on every update, update = only updated rows (used in aggregations, otherwise same as append)\n  .format(\"parquet\")\n  .queryName(\"email_traffic_query\") # optional name\n  .trigger(processingTime=\"10 second\") # how often data is fetched from source\n  .option(\"checkpointLocation\", checkpoint_path) # used for fault-tolerance. Note: every query needs to have a unique check point location\n  .start(output_path) # location where the file will be written\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"609a521d-6546-4d7b-80c3-1ead4eff0aea","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs ls tmp/email_traffic/output"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19b7ef6d-ab60-4a04-ac17-f36ea3635c2d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# monitor the query\n\n#devices_query.id # unique per query, persisted when restarted from checkpoint \n#devices_query.name\n#devices_query.status # isDataAvailable = new data available, isTriggerActive = trigger actively firing\ndevices_query.awaitTermination(5) # timeout in seconds, can use to keep cluster awake, also useful for seeing if stream was quit or got an exception\ndevices_query.stop() # note: for streaming data, cluster will keep awake (processing ongoing)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"16f07a81-32c1-401c-aec4-ce4051aa94bc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# fetching from TCP socket\n\nlogs_df = (spark.readStream\n    .format(\"socket\")\n    .option(\"host\", \"server1.databricks.training\")\n    .option(\"port\", 9001)\n    .load()\n)\n\ndisplay(logs_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73c228bd-d68a-49a0-9262-9a9b8a858925","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# we can look at a count by using the agg function\n\nrunning_count_df = logs_df.agg(F.count(\"*\"))\n\ndisplay(running_count_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dbc4490f-5e71-44a3-84ef-97db2f3c21be","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's see how comfortable it is manipulating the streaming dataframe\n# we want to get all rows where there is an error\n\n# Start by parsing out the timestamp and the log data\nclean_df = (logs_df\n            .withColumn(\"ts_string\", F.col(\"value\").substr(2, 23))\n            .withColumn(\"epoch\", F.unix_timestamp(\"ts_string\", \"yyyy/MM/dd HH:mm:ss.SSS\"))\n            .withColumn(\"capturedAt\", F.col(\"epoch\").cast(\"timestamp\"))\n            .withColumn(\"logData\", F.regexp_extract(\"value\", \"\"\"^.*\\]\\s+(.*)$\"\"\", 1))\n           )\n\n# Keep only the columns we want and then filter the data\nerrors_df = (clean_df\n             .select(\"capturedAt\", \"logData\")\n             .filter(F.col(\"value\").like(\"% (ERROR) %\"))\n            )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"557baca1-46b7-4f57-a93b-6620a84ffa65","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's write the errors dataframe into a delta table\n\n(errors_df.writeStream\n  .format(\"delta\")\n  .outputMode(\"append\")\n  .option(\"checkpointLocation\", \"/tmp/events_stream/_checkpoints/\")\n  .table(\"errors_streaming\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ffc6e36-a824-4911-a47c-9a8487cd0d02","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# we can view snapshots from this table\n\ndisplay(spark.table(\"errors_streaming\")\n        .sort(F.desc(\"capturedAt\"))\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6bc9fb39-e294-4dcc-8d52-c81f959dc9e8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# we can also check the count of this table increasing\n\nspark.table(\"errors_streaming\").count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f362145d-79e4-4ebc-8162-ff7e4e87d776","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# since it was a delta table, we can look at the history\n\nerrors_table = DeltaTable.forName(spark, \"errors_streaming\")\n\ndisplay(errors_table.history())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cd5f08ec-9d23-412b-9ba2-a2a34091ceb0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# use spark.streams.active to loop over all active streams\n# remember to stop streams if not working on them anymore\n\nfor stream in spark.streams.active:\n  stream.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ae5d3804-072b-4846-94b5-f0c99699d65a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Further reading\n\n* Delta Lake MERGE INTO:\n  * https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-merge-into.html\n* Structured Streaming: \n  * https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss.html\n  * https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html\n  * http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n  * https://docs.databricks.com/spark/latest/structured-streaming/production.html"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8b4d9fd9-1d6b-4884-ac51-afe9c2b8b32b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Task\n\nCreate a schema and a streaming dataframe for the JSON files in the following path:  \n\"/mnt/training/gaming_data/mobile_streaming_events_b/\"\n  \n  \nUse the following as basis for creating your schema:  \n |-- eventName: string (nullable = true)  \n |-- eventParams: struct (nullable = true)  \n |    |-- amount: double (nullable = true)  \n |    |-- app_name: string (nullable = true)  \n |    |-- app_version: string (nullable = true)  \n |    |-- client_event_time: string (nullable = true)  \n |    |-- device_id: string (nullable = true)  \n |    |-- game_keyword: string (nullable = true)  \n |    |-- platform: string (nullable = true)  \n |    |-- scoreAdjustment: long (nullable = true)  \n  \nRead in 2 files per trigger.\n  \nCreate a new modified dataframe:\n* keep only rows where eventName is \"scoreAdjustment\"\n* select the *game_keyword*, *platform* and *scoreAdjustment* columns from the eventParams struct.  \n* set trigger to run every 5 seconds.\n\nWrite the datastream to a delta table called score_adjustments.  \nCheck to make sure that the table has some data - this should also be visible in the cell results.  \nThen stop the datastream."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f980944-6bcd-4fdc-8081-f5efed6f66dc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# your answer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dd9cca6f-5579-4372-8a37-3dfe1ff96d2e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Practice session - Delta Lake, Structured Streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2086678445992806,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":2086678445992776}},"nbformat":4,"nbformat_minor":0}
