{"cells":[{"cell_type":"markdown","source":["### Structured Streaming\n  \n  \n* Kafka \n* Aggregations\n* Time windows\n* Watermarking\n* Joins"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8307cf20-40d9-4eef-9d0b-af50f5901b50","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pyspark.sql.functions as F"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fb203b9f-4ff6-4ebb-8c83-18b8bfadf405","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### NB: the following Kafka server seems to be taken down as of May 2023\nPlease start the code executions from cell 16"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7832de47-3c44-479f-9e67-8c851e5d4819","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# for getting data from Kafka (or other distributed log systems), we need minimum 2 things:\n# the server\n# the topic\n\nkafka_server = \"server2.databricks.training:9092\"   # US (Oregon)\n\nwiki_df = (spark.readStream                        # Get the DataStreamReader\n  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n  .option(\"subscribe\", \"en\")                       # Subscribe to the \"en\" Kafka topic - edits of English wikipedia pages\n  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n  .load()                                          # Load the DataFrame\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9b2068d2-2855-4d9b-a6e1-2e7d57853e53","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(wiki_df, streamName = \"wiki_raw_stream\")\n\n#key - the data key. Used in state machines, not useful in this case\n#value - the data, in binary format. This is our JSON payload. We'll need to cast it to STRING.\n#topic - the topic we are subscribing to\n#partition - partition. This server only has one partition.\n#offset - the offset value. This is per topic, partition, and consumer group\n#timestamp - the timestamp\n#timestampType - whether timestamp is created time or log append time"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d215cc9f-6821-49be-b0f6-1fe2480ff07c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's have a look at the JSON payload\n\nfrom pyspark.sql.types import StringType\n\nwiki_payload_df = (wiki_df\n                  .select(F.col(\"value\").cast(StringType()))\n                  )\n\ndisplay(wiki_payload_df,streamName=\"wiki_payload_stream\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"98a522a2-5e9e-4f8d-a9f2-f5db0012b652","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's create a schema for navigating the JSON payload\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n\nschema = StructType([\n  StructField(\"channel\", StringType(), True),\n  StructField(\"comment\", StringType(), True),\n  StructField(\"delta\", IntegerType(), True),\n  StructField(\"flag\", StringType(), True),\n  StructField(\"geocoding\", StructType([                 # (OBJECT): Added by the server, field contains IP address geocoding information for anonymous edit.\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"countryCode2\", StringType(), True),\n    StructField(\"countryCode3\", StringType(), True),\n    StructField(\"stateProvince\", StringType(), True),\n    StructField(\"latitude\", DoubleType(), True),\n    StructField(\"longitude\", DoubleType(), True),\n  ]), True),\n  StructField(\"isAnonymous\", BooleanType(), True),      # (BOOLEAN): Whether or not the change was made by an anonymous user\n  StructField(\"isNewPage\", BooleanType(), True),\n  StructField(\"isRobot\", BooleanType(), True),\n  StructField(\"isUnpatrolled\", BooleanType(), True),\n  StructField(\"namespace\", StringType(), True),         # (STRING): Page's namespace. See https://en.wikipedia.org/wiki/Wikipedia:Namespace \n  StructField(\"page\", StringType(), True),              # (STRING): Printable name of the page that was edited\n  StructField(\"pageURL\", StringType(), True),           # (STRING): URL of the page that was edited\n  StructField(\"timestamp\", StringType(), True),         # (STRING): Time the edit occurred, in ISO-8601 format\n  StructField(\"url\", StringType(), True),\n  StructField(\"user\", StringType(), True),              # (STRING): User who made the edit or the IP address associated with the anonymous editor\n  StructField(\"userURL\", StringType(), True),\n  StructField(\"wikipediaURL\", StringType(), True),\n  StructField(\"wikipedia\", StringType(), True),         # (STRING): Short name of the Wikipedia that was edited (e.g., \"en\" for the English)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"401c2488-760f-4374-930e-d2326fc4e632","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Now we can use \"from_json\" to parse out the message and provide schema\n\nwiki_json_df = (wiki_payload_df\n                .select(F.from_json(\"value\", schema).alias(\"json\"))\n               )\n\ndisplay(wiki_json_df,streamName=\"wiki_json_stream\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"17528c23-8656-4562-9fc7-11b2ba6eb7af","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's get only the anonymous article edits which have a location\n\nwiki_anon_df = (wiki_json_df\n  .select(F.col(\"json.wikipedia\").alias(\"wikipedia\"),      # Promoting from sub-field to column\n          F.col(\"json.isAnonymous\").alias(\"isAnonymous\"),  \n          F.col(\"json.namespace\").alias(\"namespace\"),\n          F.col(\"json.page\").alias(\"page\"),\n          F.col(\"json.pageURL\").alias(\"pageURL\"),\n          F.col(\"json.geocoding\").alias(\"geocoding\"),\n          F.col(\"json.user\").alias(\"user\"),\n          F.col(\"json.timestamp\").cast(\"timestamp\"))       # Promoting and converting to a timestamp\n  .filter(F.col(\"namespace\") == \"article\")                 # Limit result to just articles\n  .filter((F.col(\"geocoding.countryCode3\").isNotNull()))        # We only want results that are geocoded\n)\n\ndisplay(wiki_anon_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e0e5ba03-bb1c-4e66-8b62-6a480842a34d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["wiki_loc_df = (wiki_anon_df\n  .groupBy(\"geocoding.countryCode3\") # Aggregate by country (code)\n  .count()                           # Produce a count of each aggregate\n)\ndisplay(wiki_loc_df, streamName = \"location_stream\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03212514-51fb-4e54-b863-fd6077f68b87","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["(wiki_loc_df\n .writeStream\n .format(\"delta\")\n .outputMode(\"complete\") # complete overwrite on every trigger\n .option(\"checkpointLocation\", \"/tmp/wikiloc/checkpoint\")\n .table(\"wikiloc\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6d663e4-1bec-4e57-bca8-510795abca3a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.table(\"wikiloc\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7de09500-6110-4554-a1ec-898e76304986","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Time windows"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"475b45b8-9b3d-412e-9da5-0291489a5214","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# commonly, you might not want an aggregation of a stream's whole history.\n# for this purpose, let's use window from functions - NB this is \"time windows\" not \"SQL-like window function\"\n\nwiki_loc_window_df = (wiki_anon_df\n  .groupBy(\"geocoding.countryCode3\", F.window(\"timestamp\", \"5 minute\")) # Aggregate by country, every 5 minute block. This is a \"tumbling window\"\n  .count()\n)\ndisplay(wiki_loc_window_df, streamName = \"time_window_stream\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d432333a-1335-4fc4-8b5d-1e79f16ebbf9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# if we want to keep always the latest time window then we can use sliding windows\n\nwiki_loc_window_slide_df = (wiki_anon_df\n  .groupBy(\"geocoding.countryCode3\", F.window(\"timestamp\", \"5 minute\", \"1 minute\")) # Aggregate by country, 5 minute block sliding by 1 minute. This is a \"sliding window\"\n  .count()\n)\ndisplay(wiki_loc_window_slide_df, streamName = \"time_window_slide_stream\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e86c785c-51ac-4780-be1e-184e01fee543","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Non-Kafka part starts here"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"051067d9-2417-4562-bf14-5f55b0347200","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# let's try with a different, simpler dataset\n\n# %fs head dbfs:/mnt/training/sensor-data/accelerometer/time-series-stream.json/file-0.json\n\ninput_path = \"dbfs:/mnt/training/sensor-data/accelerometer/time-series-stream.json/\"\n\njson_schema = \"time timestamp, action string\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"acac1e02-d555-4fdc-aa57-0eb83b64c55d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Let's create a dataframe and apply some transformations and aggregation\n\ninput_df = (spark\n  .readStream                                 \n  .schema(json_schema)                       \n  .option(\"maxFilesPerTrigger\", 1)            \n  .json(input_path)                           \n)\n\ncounts_df = (input_df\n  .groupBy(F.col(\"action\"),                     # Aggregate by action\n           F.window(F.col(\"time\"), \"1 hour\"))     # and by a 1 hour window\n  .count()                                    # Count the actions\n  .select(F.col(\"window.start\").alias(\"start\"), \n          F.col(\"count\"),                       \n          F.col(\"action\"))                      \n  .orderBy(F.col(\"start\"))                      # Sort by the start time\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b5f36054-4522-42b8-97aa-7fabeede5da0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# displaying the output\n\ndisplay(counts_df, streamName = \"counts_stream\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"aea650a6-ed4e-44a2-8eb7-65d73326a046","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# observe the 200 partitions (Spark default) in the above query \n\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism) # reduce parallelism for aggregations and joins, set it to amount of cores. Faster processing\n\ndisplay(counts_df, streamName = \"counts_stream_improved\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5cc3b01d-587d-4fde-93e5-3ce540980f97","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Watermarking"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e200d92c-a0ce-4b7c-be79-3658e2e56dd5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# in actual use cases, the queries above would keep running for a very long time and the amount of windows would grow indefinitely\n# keeping track of all the states puts pressure on memory\n# also it may often be irrelevant if delayed data updates our figures\n\nwatermarked_stream = \"watermarked_stream\"\n\nwatermarked_df = (input_df\n  .withWatermark(\"time\", \"2 hours\")             # Specify a 2-hour watermark\n  .groupBy(F.col(\"action\"),                       # Aggregate by action...\n           F.window(F.col(\"time\"), \"1 hour\"))       # ...then by a 1 hour window\n  .count()                                      # For each aggregate, produce a count\n  .select(F.col(\"window.start\").alias(\"start\"),   # Elevate field to column\n          F.col(\"count\"),                         # Include count\n          F.col(\"action\"))                        # Include action\n  .orderBy(F.col(\"start\"))                        # Sort by the start time\n)\ndisplay(watermarked_df, streamName = watermarked_stream) # Start the stream and display it\n\n# important note: watermarking guarantees that any event within the window gets in. It does not guarantee leaving anything out."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7d968b6b-01a9-480a-8d63-b3e5c0ca159a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Let's import another dataset. Let's say we are interested in hourly monitoring of incoming traffic to our website\n\nschema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n\nhourlyEventsPath = \"/mnt/training/ecommerce/events/events-2020-07-03.json\"\n\nwebsite_df = (spark.readStream\n  .schema(schema)\n  .option(\"maxFilesPerTrigger\", 1)\n  .json(hourlyEventsPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bf4aa920-978f-4e07-ab18-efca64d10f51","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# this dataframe does not have a proper timestamp column. So we need to create one and use it for watermarking\n\nevents_df = (website_df\n             .withColumn(\"createdAt\", (F.col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n             .withWatermark(\"createdAt\", \"2 hours\")\n)             "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"81d948ed-3a26-45e7-8e92-836cc07ae9c0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# now we can do an aggregation\n\ntraffic_df = (events_df\n             .groupBy(\"traffic_source\"\n                      , F.window(F.col(\"createdAt\"), \"1 hour\"))\n             .agg(F.approx_count_distinct(\"user_id\").alias(\"active_users\"))\n             .select(F.col(\"traffic_source\")\n                     , F.col(\"active_users\")\n                     , F.hour(F.col(\"window.start\")).alias(\"hour\"))\n             .sort(\"hour\")\n)\n\ndisplay(traffic_df, streamName=\"hourly_traffic_p\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bb7b62a6-8a6d-41bc-a226-5c5d7cf40943","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Joining streams"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"307411ab-59c4-4026-b6d8-65b745ca1d28","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# let's load in users dataset\n\nusers_df = spark.read.parquet(\"/mnt/training/ecommerce/users/users.parquet/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6d2d54db-d035-4163-a9af-679884a3edb1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# join works same way as with regular dataframes.\n# note: this is streaming<->static join\n\njoined_df = (events_df\n            .join(users_df, \"user_id\")\n            )\n\ndisplay(joined_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"600645dc-8cb3-47ba-8d28-bc6330cefdb0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# example on subset of dataframe\n\njoined_limit_df = (events_df\n            .join(users_df.limit(500000), \"user_id\")\n            )\n\ndisplay(joined_limit_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d20578d4-86b4-4ef7-ac6e-818937061eea","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's read in users dataframe as a stream\n\n# since we have created the dataframe from this data, we can cheat on getting the schema. Possible in development/debugging, not possible or recommended in production\nusers_schema = users_df.schema\n\nusers_stream_df = (spark\n                   .readStream\n                   .format(\"parquet\")\n                   .schema(users_schema)\n                   .option(\"maxFilesPerTrigger\", 1)\n                   .parquet(\"/mnt/training/ecommerce/users/users.parquet/\")\n                  )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"da8f8dbd-c601-4ed6-b96a-0af6abbabf36","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's do a stream to stream join\n\njoined_streams_df = (events_df\n            .join(users_stream_df, \"user_id\")\n            )\n\ndisplay(joined_streams_df, processingTime=\"10 seconds\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"eab5049a-272a-468b-a80a-0b25830b93fe","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for stream in spark.streams.active:\n  stream.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d6ddcecc-b7c5-427e-bbd7-fea0f4886160","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Further reading\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window.html  \nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withWatermark.html  \nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html  \nhttps://docs.databricks.com/spark/latest/structured-streaming/index.html"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e2ffb003-abb3-4689-9ad0-8564ac25e1cb","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Task 1\n\nCreate a streaming dataframe from the data in the following path:  \n/mnt/training/asa/flights/2007-01-stream.parquet/\n\nThe schema should contain\n* DepartureAt (timestamp)\n* UniqueCarrier (string)\n\nProcess only 1 file per trigger.  \n\nAggregate the data by count, using non-overlapping 30 minute windows.  \nIgnore any data that is older than 6 hours.\n\nThe output should have 3 columns: startTime (window start time), UniqueCarrier, count.  \nThe output should be sorted ascending by startTime.\n\nDisplay the output, firing the trigger every 5 seconds.\n\nOnce the stream has produced some output, call the stream shutdown function."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b127445-2b2f-49fb-84fe-fc9063f2450f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Your answer\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4204d127-eb1f-4c1f-be71-dd4c638935f2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Practice session - Structured Streaming continued","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3621743426785043}},"nbformat":4,"nbformat_minor":0}
