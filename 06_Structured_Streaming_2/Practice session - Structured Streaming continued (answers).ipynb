{"cells":[{"cell_type":"markdown","source":["### Task 1\n\nCreate a streaming dataframe from the data in the following path:  \n/mnt/training/asa/flights/2007-01-stream.parquet/\n\nThe schema should contain\n* DepartureAt (timestamp)\n* UniqueCarrier (string)\n\nProcess only 1 file per trigger.  \n\nAggregate the data by count, using non-overlapping 30 minute windows.  \nIgnore any data that is older than 6 hours.\n\nThe output should have 3 columns: startTime (window start time), UniqueCarrier, count.  \nThe output should be sorted ascending by startTime.\n\nDisplay the output, firing the trigger every 5 seconds.\n\nOnce the stream has produced some output, call the stream shutdown function."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b127445-2b2f-49fb-84fe-fc9063f2450f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# ANSWER\nimport pyspark.sql.functions as F\n\npath = \"/mnt/training/asa/flights/2007-01-stream.parquet/\"\n\nschema = \"DepartureAt timestamp, UniqueCarrier string\"\n\nstreamDF = (spark                   \n  .readStream                       \n  .format(\"parquet\")                \n  .schema(schema)            \n  .option(\"maxFilesPerTrigger\", 1)  \n  .load(path)                   \n)\n\ncountsDF = (streamDF                                             # Start with the DataFrame\n  .withWatermark(\"DepartureAt\", \"300 minutes\")                   # Specify the watermark\n  .groupBy(F.window(\"DepartureAt\", \"30 minute\"),\"UniqueCarrier\") # Aggregate the data\n  .count()                                                       # Produce a count for each aggreate\n  .withColumn(\"startTime\", F.col(\"window.start\"))                # Add the column \"startTime\", extracting it from \"window.start\"\n  .drop(\"window\")                                                # Drop the window column\n  .orderBy(\"startTime\")                                          # Sort the stream by \"startTime\" \n)\ndisplay(countsDF, processingTime = \"5 seconds\", streamName = \"myStreamName\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4204d127-eb1f-4c1f-be71-dd4c638935f2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0},{"cell_type":"code","source":["for stream in spark.streams.active:\n  if stream.name == \"myStreamName\":\n    stream.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"635b0f21-5bfd-40a1-b6c8-3cc77eefec20","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Practice session - Structured Streaming (answers)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2687514412223128}},"nbformat":4,"nbformat_minor":0}
