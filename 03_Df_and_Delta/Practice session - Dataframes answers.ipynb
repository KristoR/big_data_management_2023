{"cells":[{"cell_type":"markdown","source":["### Task 1\n\nBased on the sales dataframe used above, create a new dataframe that has the following fields:\n* order_id\n* email\n* purchase_revenue_in_usd\n* value_added_tax\n\nValue_added_tax should be an UDF, calculated as follows:\n* if purchase_revenue_in_usd > 1500: tax = revenue*0.2\n* else: tax = revenue*0.1\n\nThe dataframe should be filtered to only include rows where value_added_tax is above 110.\n\nThe dataframe should be returned in a descending order based on the value_added_tax column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ef2ab17-8de4-40d2-bf5a-2597e19eb30f"}}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\nimport pyspark.sql.functions as F\n\nsales_df = spark.read.parquet(\"/mnt/training/ecommerce/sales/sales.parquet\")\n\ndef value_added_tax(revenue):\n  if revenue > 1500:\n    return revenue * 0.2\n  else:\n    return revenue * 0.1\n\nvat = udf(value_added_tax, DoubleType())\n\nvat_df = (sales_df.select(\"order_id\", \"email\", \"purchase_revenue_in_usd\")\n  .withColumn(\"value_added_tax\", vat(\"purchase_revenue_in_usd\"))\n  .filter(\"value_added_tax > 110\")\n  .orderBy(F.desc(\"value_added_tax\")))\n              \ndisplay(vat_df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdb43a5b-371a-49ab-9345-569b7e4fdcb5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Task 2\n\nJoin the events dataframe from above and the zips dataset at path:</br>\n/mnt/training/zips.json\n\nThe join should be done based on city and state. Note that the join is case-sensitive, so transform the columns accordingly before the join.\n\nReturn a dataframe which has the following columns:\n* user_id\n* latitude\n* longitude\n\nLatitude is element 2 in the \"loc\" column of the zips dataset</br>\nLongitude is element 1 in the \"loc\" column of the zips dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76b44f99-23bd-4813-83b2-3a9dd70f84a5"}}},{"cell_type":"code","source":["events_df = (spark.read\n             .option(\"inferSchema\", True)\n             .json(\"/mnt/training/ecommerce/events/events-500k.json\")\n             )\n\nzips_df = (spark.read\n            .option(\"inferSchema\", True)\n            .json(\"/mnt/training/zips.json\")\n            )\n\nresult_df = (events_df.select(\"user_id\"\n                             , F.upper(\"geo.city\").alias(\"city\")\n                             , F.upper(\"geo.state\").alias(\"state\")\n                            )\n             .join(\n               zips_df.select(F.upper(\"city\").alias(\"city\")\n                              , F.upper(\"state\").alias(\"state\")\n                              , F.element_at(F.col(\"loc\"), 2).alias(\"latitude\")\n                              , F.element_at(F.col(\"loc\"), 1).alias(\"longitude\")\n                             )\n               , [\"city\", \"state\"]\n             )\n             .drop(\"city\", \"state\")\n            )\n      \ndisplay(result_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58be407f-90bf-4ba6-a2f3-83902b60ad44"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Dataframes answers","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1360913719108538}},"nbformat":4,"nbformat_minor":0}
