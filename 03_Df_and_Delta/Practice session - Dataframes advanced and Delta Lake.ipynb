{"cells":[{"cell_type":"markdown","source":["### Dataframes advanced + Delta Lake\n\n* Dataframes\n    * Custom schemas\n    * Pivot\n    * Window functions\n    * Delta Lake"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b131d293-c37a-4753-95e3-85a755db6aaa","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Custom schema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5787966d-8ae7-4a2c-a7f2-2199099809f6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Let's import a JSON dataset and have a look at the file\n\nimport requests\nr = requests.get(\"https://think.cs.vt.edu/corgis/datasets/json/airlines/airlines.json\")\n\nprint(r.json()[:2])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a605b371-d189-4bc6-9eb6-7bed18b9b2d4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# We can also make json into Row objects\nfrom pyspark.sql import Row\n\nrows = (Row(**x) for x in r.json())\n\nfor row in rows:\n  print(row)\n  break"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"676417d5-1fa4-4563-b466-fab61135b8ce","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# creating a dataframe from this JSON string (Python dictionary) gives us many nested MapType columns\n\nairlines_df = spark.createDataFrame(r.json()) # Inferring schema from Python dict was deemed to be deprecated, but is now again accepted starting from Spark 3.1\nairlines_df2 = spark.createDataFrame(Row(**x) for x in r.json()) # another way of creating df from dict: unpacking each JSON element into a spark Row element\n\ndisplay(airlines_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b7c22a9e-a4da-4b73-8ab2-128d0d9345f2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's test some functions on map columns\n\nimport pyspark.sql.functions as F\n\ndisplay(airlines_df\n        .select(F.explode(\"Airport\") # we can try explode - but this does not make sense for this kind of data\n                #F.col(\"Airport\").getItem(\"Code\").alias(\"AirportCode\") # we can use getItem to fetch values per key - good for small map columns, not good for large/nested map columns\n                #,F.col(\"Airport\").getItem(\"Name\").alias(\"AirportName\")\n                #,F.col(\"Airport\")[\"Code\"] # alternative way, referencing Python dictionary style\n                #F.col(\"Airport.Code\"),F.col(\"Airport.Name\") # alternative, easier to call but may confuse as it looks like struct. But Airport.* does not work for map\n                #F.map_keys(\"Airport\") # array of keys\n                #,F.map_values(\"Airport\") # array of values\n                ,\"*\")\n        )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d10699ed-12db-4f47-a18b-57aefe56caa1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# usually, a struct type of column is more useful and easier to access\n# in this case, we need to custom define a schema when reading in the DataFrame.\n# this is also recommended for real-life data pipelines, especially in case of large amount of small data files\n# potential down-side: missing schema evolution\n\nfrom pyspark.sql.types import *\n\n# The outer part needs to be a StructType\n# A StructType needs to consist of StructFields\n# StructFields have 3 parameters: name, type, nullable\n\n# Note: you can remove or add parts of schema\n# Note2: name has to match the key/column name in the dataset.\n\nairport_schema = StructType([\n  StructField(\"Airport\",StructType([\n    StructField(\"Code\", StringType(), True),\n    StructField(\"Name\", StringType(), True)\n  ]),True), \n  StructField(\"Statistics\",StructType([\n    StructField(\"Carriers\", StructType([\n      StructField(\"Names\", StringType(), True),\n      StructField(\"Total\", IntegerType(), True)\n    ]), True),\n    StructField(\"Minutes Delayed\", StructType([\n      StructField(\"Late Aircraft\", LongType(), True),\n      StructField(\"National Aviation System\", LongType(), True),\n      StructField(\"Weather\", LongType(), True),\n      StructField(\"Carrier\", LongType(), True),\n      StructField(\"Security\", LongType(), True),\n      StructField(\"Total\", LongType(), True)\n    ]), True),\n    StructField(\"Flights\", StructType([\n      StructField(\"Delayed\", LongType(), True),\n      StructField(\"Diverted\", LongType(), True),\n      StructField(\"Cancelled\", LongType(), True),\n      StructField(\"On Time\", LongType(), True),\n      StructField(\"Total\", LongType(), True)\n    ]), True),\n    StructField(\"# of Delays\", StructType([\n      StructField(\"Late Aircraft\", LongType(), True),\n      StructField(\"National Aviation System\", LongType(), True),\n      StructField(\"Weather\", LongType(), True),\n      StructField(\"Carrier\", LongType(), True),\n      StructField(\"Security\", LongType(), True)\n    ]), True)\n  ]),True),\n  StructField(\"Time\",StructType([\n    StructField(\"Label\", StringType(), True),\n    StructField(\"Month\", IntegerType(), True),\n    StructField(\"Year\", IntegerType(), True),\n    StructField(\"Month Name\", StringType(), True)\n  ]),True)\n  #StructField(\"MyNullTimestamp\", TimestampType(), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2088142-318a-4ccf-a9d6-9c81106753e2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# we can now provide this schema as input in our dataframe creation\n\nairport_schema_df = spark.createDataFrame(r.json(), schema=airport_schema) #spark is not inferring schema. This generally runs much faster for big data\ndisplay(airport_schema_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43f3c3f5-013c-4972-93c8-90f4b1c834d0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# another way to create a schema is using StructType's \"add\" method\n\nairport_add_schema = (StructType()\n                      .add(\"Airport\", StructType()\n                          .add(\"Code\", StringType())\n                          .add(\"Name\", StringType())\n                          )\n                      .add(\"Time\", StructType()\n                          .add(\"Month\",IntegerType())\n                          .add(\"Year\",IntegerType()))\n                     )\n\nairport_add_schema_df = spark.createDataFrame(r.json(), schema=airport_add_schema)\ndisplay(airport_add_schema_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c166420d-84e2-4687-84e7-7e80bf22b509","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# third method, raw string\nairport_string_schema = \"Airport STRUCT<Code: STRING, Name: STRING>, Time STRUCT<Month: INTEGER, Year: INTEGER, Date: INTEGER>\" # date will be null, just an example of adding columns which don't exist\n\nairport_string_schema_df = spark.createDataFrame(r.json(), schema=airport_string_schema) \ndisplay(airport_string_schema_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3de399ef-7573-4125-909b-8d9bc8d4a5d6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# It is now much easier to navigate and manipulate the fields.\n\ndisplay(airport_schema_df\n        .select(\"Airport.*\"\n               ,\"Statistics.*\"\n               ,\"*\"\n               )\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e062ad80-2131-4259-91de-fcd6fe4def36","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Pivot"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"61fef9e0-3794-47e6-8edf-15c469ea99db","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# pivot table - summarizing a more extensive table, i.e. plotting data points as columns\n# let's first load in a dataset\n\nairbnb_df = spark.read.parquet(\"mnt/training/airbnb/amsterdam-listings/amsterdam-listings-2018-12-06.parquet/*.parquet\")\ndisplay(airbnb_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d63649e2-8a56-4082-bf9f-24e716f4eeee","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# the dataset has many columns. Let's say we are interested in the average prices per city and neighbourhood.\n# we are also interested in the size of the place - how many people it accommodates\n\ndisplay(airbnb_df.select(\"city\"\n                       , \"neighbourhood\"\n                       , \"accommodates\"\n                       , \"price\")\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e43851a6-f96a-4d36-9754-1e052a2e7dff","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(airbnb_df\n        .select(\"city\", \"neighbourhood\", \"accommodates\", \"price\")\n        .groupby(\"city\", \"neighbourhood\") # \"row\" \n        .pivot(\"accommodates\") # columns\n        .mean(\"price\") # data / values     # possible options: mean, sum, min, max, count\n        .na.fill(0) # for filling out null values. 0 for counts/sums\n        .orderBy(F.desc(\"2\")) # for ordering\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30cf918b-6f60-451a-9d91-af362dfa258c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# another pivot example\n\ndf_wiki = spark.read.parquet(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet/*.parquet\")\n\ndisplay(df_wiki)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eaf9a0dc-15d1-4e2b-8010-6200c806a786","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(df_wiki\n        .selectExpr(\"cast(timestamp as date) as date\"\n                   ,\"hour(timestamp) as hour\"\n                   ,\"site\"\n                   ,\"requests\")\n        .groupBy(\"date\",\"hour\") #\"date\"\n        .pivot(\"site\")\n        .sum(\"requests\")\n        .orderBy(\"date\",\"hour\") #\"date\" \n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"74b9a8b0-01bb-4faf-a259-480d6ef292c9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### SQL window functions\n\n_Note: a \"window\" can be many different things. Here we talk about classical SQL window functions_"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d241577-126c-4ddb-b0a9-f1b85010ed32","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# let's load in a new dataset and have a look at it\nhealthcare_df = spark.read.parquet(\"/mnt/training/healthcare/tracker/health_profile_data.snappy.parquet\")\ndisplay(healthcare_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a1988f5d-bb39-43be-83e3-0ab05b99505c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# we need to import the Window API and instantiate a Window specification object\n# we need also pyspark.sql.functions (F) for using aggregations and functions  \n\nfrom pyspark.sql import Window\n\nwindow_spec = Window.partitionBy(\"_id\").orderBy(\"dte\") \n\ndisplay(healthcare_df\n       .withColumn(\"row_num\", F.row_number().over(window_spec)) # similarly can use rank and dense_rank\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"226bb17c-c10c-4340-ad51-3700ffeae812","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# you can use multiple window specs in parallel\n  \nwindow_spec_by_hr = Window.partitionBy(\"_id\").orderBy(\"resting_heartrate\") #for descending, you can use .orderBy(F.desc(\"resting_heartrate\"))\n\ndisplay(healthcare_df\n       .withColumn(\"row_num\", F.row_number().over(window_spec)) # similarly can use rank and dense_rank\n       .withColumn(\"rank_num\", F.rank().over(window_spec_by_hr))\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"442522a1-4b5c-41e1-b4dc-3b1feb95d374","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# use lag / lead for viewing \"back\" or \"ahead\" within a partition's rows\n\ndisplay(healthcare_df\n       .withColumn(\"lag_resting_heartrate\", F.lag(\"resting_heartrate\").over(window_spec)) # use for getting previous/next value in partition. \n       .withColumn(\"lead_resting_heartrate\", F.lead(\"resting_heartrate\", 5, 0).over(window_spec)) # Can define offset and default value\n       .withColumn(\"diffToPrev\",F.expr(\"resting_heartrate - lag_resting_heartrate\")) # useful for getting the increase/decrease\n       )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cad161ca-e70f-4ac5-8bca-b01a2e44496f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# rolling windows - useful for moving averages, rolling total, etc\n\nwindow_spec_rolling = Window.partitionBy(\"_id\").orderBy(\"dte\").rowsBetween(Window.unboundedPreceding, Window.currentRow) # rolling aggregations - everything up to current row\nwindow_spec_rolling_last_week = Window.partitionBy(\"_id\").orderBy(\"dte\").rowsBetween(-6, Window.currentRow) # rolling aggregations, use negative integer for previous rows\nwindow_spec_rolling_plusmin2 = Window.partitionBy(\"_id\").orderBy(\"dte\").rowsBetween(-2, 2) # rolling aggregations, use positive integer for next rows\n\ndisplay(healthcare_df\n       .withColumn(\"row_num\", F.row_number().over(window_spec)) \n       .withColumn(\"rolling_avg\", F.avg(\"resting_heartrate\").over(window_spec_rolling_last_week)) # using aggregations with window functions\n       .withColumn(\"row_num_sum\", F.sum(\"row_num\").over(window_spec_rolling_plusmin2))\n       .withColumn(\"max_BMI_3\", F.max(\"BMI\").over(window_spec_rolling_plusmin2))\n       )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f7792582-a539-4c3f-a0b8-2b3ce9c0fff8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Delta Lake"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"655b6c16-409e-4112-a9be-bbc50fc5b040","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# let's load in a small df for demonstration purposes\n\niso_df = (spark.read\n          .option(\"header\",\"true\")\n          .option(\"inferSchema\",\"true\")\n          .csv(\"/mnt/training/countries/ISOCountryCodes/ISOCountryLookup.csv\")\n         )\ndisplay(iso_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7d1a2200-97e5-4950-b271-f4cc9e589c6c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# General issue with data lakes / Hive tables / HDFS storage\n# Hard to do SQL / Data Warehouse-like updates\n# No ACID compliance (Hive has now in newer versions, but not inherently compatible with Spark)\n\n# Delta Lake = open source\n# ACID, time-travel, optimized performance, ...\n\niso_df.write.format(\"hive\").mode(\"overwrite\").saveAsTable(\"hive_iso_t\") # default outside of Databricks without format\niso_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_iso_t\") "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ceebbea-7f30-4259-aa9b-c6cb1dcfc4d4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.table(\"hive_iso_t\"))\n#display(spark.table(\"delta_iso_t\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"20fe7ef3-4fca-455e-a74d-09d84fa8adf8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# updating using spark sql statements\nspark.sql(\"UPDATE hive_iso_t SET independentTerritory = 'Yes' WHERE EnglishShortName = 'Antarctica'\") # fails, update not supported\n#spark.sql(\"UPDATE delta_iso_t SET independentTerritory = 'Yes' WHERE EnglishShortName = 'Antarctica'\") # OK\n\ndisplay(spark.table(\"delta_iso_t\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ed135f4-1bcf-4041-88a8-aa5c278b9307","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# another way is to use delta API, useful for more advanced usecases and simpler programmability\n\nfrom delta.tables import *\n\ndelta_table = DeltaTable.forName(spark, \"delta_iso_t\") \n\ndelta_table.update(\n  condition = \"EnglishShortName = 'Greenland'\",\n  set = { \"independentTerritory\": \"'Yes'\"}\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"28aa1aac-b11f-4572-b366-85b020cf5807","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.sql(\"DESCRIBE HISTORY delta_iso_t\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2a853029-1855-4276-8a87-ff2c61409b8b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# view history of table\n\ndisplay(delta_table.history()) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"32c348dd-9a10-42ba-add0-dee1b14fde8f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# creating dataframe from previous state\n\n#display(spark.sql(\"DESCRIBE EXTENDED delta_iso_t\")) # getting the table path\n\n#timestamp_df = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2021-03-26 13:17:00\").load(\"/user/hive/warehouse/delta_iso_t\")\n#display(timestamp_df)\n#version_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/user/hive/warehouse/delta_iso_t\")\n#display(version_df)\n\n# restoring table to previous state\n\n#delta_table.restoreToTimestamp('2021-03-26 13:17:00') # restore to a specific timestamp\n#delta_table.restoreToVersion(0) # restore table to (oldest) version\n#display(spark.table(\"delta_iso_t\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19772ea0-d0ed-4de9-a228-449fb6153398","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Further reading\n\n* Spark SQL Window functions\n  * https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html\n* Delta Lake: \n  * https://delta.io/\n  * https://docs.delta.io/latest/api/python/index.html\n  * https://medium.com/datalex/5-reasons-to-use-delta-lake-format-on-databricks-d9e76cf3e77d"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8b4d9fd9-1d6b-4884-ac51-afe9c2b8b32b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Tasks for session 3"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c6c65d7a-9c3e-4c18-b886-140896a5e76a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Task 1\n\nEmployees' dataset: \"/mnt/training/manufacturing-org/employees/employees.csv\"\n\nUsing window functions, find the **employees** who have worked in a specific **department** the *longest* and *shortest* time.\n\nResulting dataframe should have 3 columns: employee_name, department, employment_duration\n\nEmployment_duration should have 2 possible values: \n* **longest**\n  * The employee has worked in the department for the longest time. Based on column _active_record_start_\n* **shortest**\n  * The employee has worked in the department for the shortest time. Based on column _active_record_start_\n\nResulting dataframe should have total 6 rows.\n\nExample df.take(3):</br>\n<table>\n  <tr>\n    <th>employee_name</th>\n    <th>department</th>\n    <th>employment_duration</th>\n  </tr>\n  <tr>\n    <td>CISNEROS JR, HERBERT</td>\n    <td>OFFICE</td>\n    <td>shortest</td>\n  </tr>\n  <tr>\n    <td>CRAVEN, KEVIN J</td>\n    <td>OFFICE</td>\n    <td>longest</td>\n  </tr>\n  <tr>\n    <td>WRIGHT, RONALD G</td>\n    <td>PRODUCTION</td>\n    <td>shortest</td>\n  </tr>\n</table>\n\nNote: </br>\nShould be doable with 2 window functions and 2 transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75173e12-22f5-4848-b44f-e62e34597d05","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# your answer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0afee002-3810-4c66-a8f7-cbc3113a32ca","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Practice session - Dataframes advanced and Delta Lake","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":528336678317194,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":3667371382687778}},"nbformat":4,"nbformat_minor":0}
